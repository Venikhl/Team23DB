{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0635285d-5589-4eeb-9091-9dd2143b3492",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d4d77bfb-132e-422c-9933-69d0df5685dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[253]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Location of your Hive database in HDFS\u001b[39;00m\n\u001b[32m      7\u001b[39m warehouse = \u001b[33m\"\u001b[39m\u001b[33mproject/hive/warehouse\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m - spark ML\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myarn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhive.metastore.uris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthrift://hadoop-02.uni.innopolis.ru:9883\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarehouse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.avro.compression.codec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msnappy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43menableHiveSupport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33msession started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Now you can proceed with your Spark operations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:503\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    500\u001b[39m     session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(session._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     ).applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1712\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team23\"\n",
    "\n",
    "# Location of your Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "print(\"session started\")\n",
    "# Now you can proceed with your Spark operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a0875-0d8c-4f48-a65b-230e11c46d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/IPython/core/formatters.py:406\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    404\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:617\u001b[39m, in \u001b[36mSparkSession._repr_html_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_html_\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    611\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[33m        <div>\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[33m            <p><b>SparkSession - \u001b[39m\u001b[38;5;132;01m{catalogImplementation}\u001b[39;00m\u001b[33m</b></p>\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[33m            \u001b[39m\u001b[38;5;132;01m{sc_HTML}\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33m        </div>\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m.format(\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m         catalogImplementation=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.catalogImplementation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m    618\u001b[39m         sc_HTML=\u001b[38;5;28mself\u001b[39m.sparkContext._repr_html_(),\n\u001b[32m    619\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/conf.py:54\u001b[39m, in \u001b[36mRuntimeConfig.get\u001b[39m\u001b[34m(self, key, default)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m._checkType(key, \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff80e843a10>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc89ee-c475-47d1-b000-a800563a8870",
   "metadata": {},
   "source": [
    "# list Hive databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2631136-e462-4815-a807-d1bd4bc3f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(spark.catalog.listDatabases())\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a1fe4-d2d3-4fb0-9533-59f61a917d05",
   "metadata": {},
   "source": [
    "# Specify the input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018f48d1-6fde-4ff5-b7ce-4eed719a7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following features\n",
    "# Excluded 'comm' because it has a lot of nulls\n",
    "# Excuded hiredate because it is given as practice to implement the cos_sin_transformation for the student\n",
    "features = [\n",
    "    \"vendor_id\", \n",
    "    \"passenger_count\", \n",
    "    \"trip_distance\", \n",
    "    \"rate_code\", \n",
    "    \"payment_type\", \n",
    "    \"pickup_location_id\", \n",
    "    \"dropoff_location_id\",\n",
    "    \"year\",  # keep as numerical\n",
    "    # cyclical encoded\n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"day_sin\", \"day_cos\",\n",
    "    \"hour_sin\", \"hour_cos\",\n",
    "    \"minute_sin\", \"minute_cos\",\n",
    "    \"second_sin\", \"second_cos\",\n",
    "]\n",
    "\n",
    "# The output/target of our model\n",
    "label = \"tip_amount\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bf565-af4e-49d4-bf45-ca1e1baf971f",
   "metadata": {},
   "source": [
    "# Read hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3376ccfd-3045-4c2d-9800-19ea2da702b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = spark.read.format(\"avro\").table('team23_projectdb.trips_part_buck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ee445f-9548-45d2-bf0c-c652085aae36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_id', 'int'),\n",
       " ('vendor_id', 'int'),\n",
       " ('pickup_datetime', 'bigint'),\n",
       " ('dropoff_datetime', 'bigint'),\n",
       " ('passenger_count', 'int'),\n",
       " ('trip_distance', 'string'),\n",
       " ('rate_code', 'int'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('payment_type', 'int'),\n",
       " ('fare_amount', 'string'),\n",
       " ('extra', 'string'),\n",
       " ('mta_tax', 'string'),\n",
       " ('tip_amount', 'string'),\n",
       " ('tolls_amount', 'string'),\n",
       " ('imp_surcharge', 'string'),\n",
       " ('total_amount', 'string'),\n",
       " ('pickup_location_id', 'int'),\n",
       " ('dropoff_location_id', 'int'),\n",
       " ('pickup_date', 'string')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662a1c70-3549-484c-99b9-00631ccfbc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 16:33:27 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------------+----------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----------+\n",
      "|trip_id|vendor_id|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|imp_surcharge|total_amount|pickup_location_id|dropoff_location_id|pickup_date|\n",
      "+-------+---------+---------------+----------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----------+\n",
      "|9282024|        2|  1514728527000|   1514729473000|              4|         8.29|        1|                 N|           1|      24.50| 0.50|   0.50|      5.16|        0.00|         0.30|       30.96|               230|                243|    2017-12|\n",
      "|9463218|        2|  1514708541000|   1514710139000|              4|         5.67|        1|                 N|           1|      22.00| 0.50|   0.50|      4.66|        0.00|         0.30|       27.96|               114|                181|    2017-12|\n",
      "|9459716|        2|  1514752088000|   1514753786000|              2|         5.32|        1|                 N|           1|      22.00| 0.50|   0.50|      2.33|        0.00|         0.30|       25.63|               170|                 37|    2017-12|\n",
      "|3310689|        2|  1514753880000|   1514754783000|              1|        11.12|        1|                 N|           1|      30.50| 0.50|   0.50|      4.20|        0.00|         0.30|       36.00|               132|                160|    2017-12|\n",
      "|6549226|        2|  1514750825000|   1514752345000|              1|        10.88|        1|                 N|           2|      31.00| 0.50|   0.50|      0.00|        0.00|         0.30|       32.30|                43|                 32|    2017-12|\n",
      "|7103045|        2|  1514753848000|   1514754897000|              1|         7.81|        1|                 N|           1|      23.00| 0.50|   0.50|     10.00|        0.00|         0.30|       34.30|               229|                 40|    2017-12|\n",
      "|1465281|        2|  1514734050000|   1514735323000|              3|         7.51|        1|                 N|           2|      24.00| 0.50|   0.50|      0.00|        5.76|         0.30|       31.06|               186|                226|    2017-12|\n",
      "|2210771|        2|  1514728130000|   1514729434000|              1|         7.74|        1|                 N|           1|      24.00| 0.50|   0.50|      0.05|        0.00|         0.30|       25.35|               181|                 79|    2017-12|\n",
      "|1207943|        2|  1514752047000|   1514837019000|              2|         6.01|        1|                 N|           1|      25.00| 0.50|   0.50|      7.89|        0.00|         0.30|       34.19|               144|                239|    2017-12|\n",
      "|1166045|        2|  1514753979000|   1514755203000|              1|         8.06|        1|                 N|           1|      25.50| 0.50|   0.50|      6.51|        5.76|         0.30|       39.07|               138|                166|    2017-12|\n",
      "| 510902|        2|  1514753119000|   1514754224000|              1|        11.91|        1|                 N|           1|      32.50| 0.50|   0.50|      6.76|        0.00|         0.30|       40.56|               138|                 40|    2017-12|\n",
      "| 321726|        2|  1514753899000|   1514754936000|              2|         9.94|        1|                 N|           1|      28.00| 0.50|   0.50|      7.00|        5.76|         0.30|       42.06|               138|                237|    2017-12|\n",
      "|8698920|        2|  1514752821000|   1514837353000|              2|        11.19|        1|                 N|           2|      33.50| 0.50|   0.50|      0.00|        0.00|         0.30|       34.80|               234|                243|    2017-12|\n",
      "|7986535|        2|  1514729097000|   1514814302000|              2|         8.70|        1|                 N|           1|      32.00| 0.50|   0.50|      7.81|        5.76|         0.30|       46.87|               100|                181|    2017-12|\n",
      "|5501883|        2|  1514710845000|   1514796006000|              2|         2.61|        1|                 N|           2|      22.50| 0.50|   0.50|      0.00|        0.00|         0.30|       23.80|               163|                 68|    2017-12|\n",
      "|5894750|        2|  1514753957000|   1514755280000|              1|         7.00|        1|                 N|           1|      23.00| 0.50|   0.50|      4.70|        0.00|         0.30|       29.00|               233|                 36|    2017-12|\n",
      "|5573376|        2|  1514749109000|   1514834147000|              2|         8.01|        1|                 N|           1|      30.00| 0.50|   0.50|      3.00|        5.76|         0.30|       40.06|               164|                181|    2017-12|\n",
      "|4131269|        2|  1514745283000|   1514829413000|              6|        13.69|        1|                 N|           1|      38.50| 0.50|   0.50|      0.00|        0.00|         0.30|       39.80|               148|                 16|    2017-12|\n",
      "|3806217|        2|  1514752373000|   1514836760000|              2|         8.85|        1|                 N|           1|      28.50| 0.50|   0.50|      5.96|        0.00|         0.30|       35.76|               166|                232|    2017-12|\n",
      "|4501701|        2|  1514728610000|   1514813402000|              1|         6.42|        1|                 N|           1|      25.50| 0.50|   0.50|      6.70|        0.00|         0.30|       33.50|               186|                 33|    2017-12|\n",
      "+-------+---------+---------------+----------------+---------------+-------------+---------+------------------+------------+-----------+-----+-------+----------+------------+-------------+------------+------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e05e6d-b953-4dd6-84ae-dbeac729dc76",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b92255-0bfc-4500-850f-71baee8383a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Null / Missing Values Check ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " trip_id             | 0   \n",
      " vendor_id           | 0   \n",
      " pickup_datetime     | 0   \n",
      " dropoff_datetime    | 0   \n",
      " passenger_count     | 0   \n",
      " trip_distance       | 0   \n",
      " rate_code           | 0   \n",
      " store_and_fwd_flag  | 0   \n",
      " payment_type        | 0   \n",
      " fare_amount         | 0   \n",
      " extra               | 0   \n",
      " mta_tax             | 0   \n",
      " tip_amount          | 0   \n",
      " tolls_amount        | 0   \n",
      " imp_surcharge       | 0   \n",
      " total_amount        | 0   \n",
      " pickup_location_id  | 0   \n",
      " dropoff_location_id | 0   \n",
      " pickup_year         | 0   \n",
      " pickup_month        | 0   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 16:34:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Summary Statistics ====\n",
      "\n",
      "\n",
      "Count:\n",
      "trip_id : 10000000\n",
      "vendor_id : 10000000\n",
      "pickup_datetime : 10000000\n",
      "dropoff_datetime : 10000000\n",
      "passenger_count : 10000000\n",
      "trip_distance : 10000000\n",
      "rate_code : 10000000\n",
      "store_and_fwd_flag : 10000000\n",
      "payment_type : 10000000\n",
      "fare_amount : 10000000\n",
      "extra : 10000000\n",
      "mta_tax : 10000000\n",
      "tip_amount : 10000000\n",
      "tolls_amount : 10000000\n",
      "imp_surcharge : 10000000\n",
      "total_amount : 10000000\n",
      "pickup_location_id : 10000000\n",
      "dropoff_location_id : 10000000\n",
      "pickup_year : 10000000\n",
      "pickup_month : 10000000\n",
      "\n",
      "Mean:\n",
      "trip_id : 5000000.5\n",
      "vendor_id : 1.6143277\n",
      "pickup_datetime : 1.5298667605022021E12\n",
      "dropoff_datetime : 1.5298689075286394E12\n",
      "passenger_count : 1.6029494\n",
      "trip_distance : 8.84928020999992\n",
      "rate_code : 1.2012385\n",
      "store_and_fwd_flag : None\n",
      "payment_type : 1.1892995\n",
      "fare_amount : 31.652551951000152\n",
      "extra : 0.33837806500000006\n",
      "mta_tax : 0.481928941\n",
      "tip_amount : 5.5985269889998746\n",
      "tolls_amount : 2.1379179970021194\n",
      "imp_surcharge : 0.29788313800085975\n",
      "total_amount : 40.51607352905553\n",
      "pickup_location_id : 153.5631699\n",
      "dropoff_location_id : 148.1428256\n",
      "pickup_year : 2017.9998715\n",
      "pickup_month : 6.2761029\n",
      "\n",
      "Stddev:\n",
      "trip_id : 2886751.490285687\n",
      "vendor_id : 0.5146575594508939\n",
      "pickup_datetime : 9.095111620217701E9\n",
      "dropoff_datetime : 9.09519466751886E9\n",
      "passenger_count : 1.2457816727809203\n",
      "trip_distance : 5.882028461543538\n",
      "rate_code : 1.2507334338503844\n",
      "store_and_fwd_flag : None\n",
      "payment_type : 0.43398757831794127\n",
      "fare_amount : 160.60113941564492\n",
      "extra : 0.5512911467900136\n",
      "mta_tax : 0.12072823130547782\n",
      "tip_amount : 4.840596207475438\n",
      "tolls_amount : 3.750309296286839\n",
      "imp_surcharge : 0.03406027438649299\n",
      "total_amount : 161.16336870776715\n",
      "pickup_location_id : 60.76455590224333\n",
      "dropoff_location_id : 75.74852785257227\n",
      "pickup_year : 0.0389035170530679\n",
      "pickup_month : 3.423658140170408\n",
      "\n",
      "Min:\n",
      "trip_id : 1\n",
      "vendor_id : 1\n",
      "pickup_datetime : 978296824000\n",
      "dropoff_datetime : 978296850000\n",
      "passenger_count : 0\n",
      "trip_distance : 0.0\n",
      "rate_code : 1\n",
      "store_and_fwd_flag : N\n",
      "payment_type : 1\n",
      "fare_amount : -800.0\n",
      "extra : -80.0\n",
      "mta_tax : -0.5\n",
      "tip_amount : -322.42\n",
      "tolls_amount : -52.5\n",
      "imp_surcharge : -0.3\n",
      "total_amount : -800.3\n",
      "pickup_location_id : 1\n",
      "dropoff_location_id : 1\n",
      "pickup_year : 2001\n",
      "pickup_month : 1\n",
      "\n",
      "Max:\n",
      "trip_id : 10000000\n",
      "vendor_id : 4\n",
      "pickup_datetime : 2635842333000\n",
      "dropoff_datetime : 2635842333000\n",
      "passenger_count : 9\n",
      "trip_distance : 7655.76\n",
      "rate_code : 99\n",
      "store_and_fwd_flag : Y\n",
      "payment_type : 5\n",
      "fare_amount : 398460.05\n",
      "extra : 84.0\n",
      "mta_tax : 150.0\n",
      "tip_amount : 496.0\n",
      "tolls_amount : 918.25\n",
      "imp_surcharge : 1.0\n",
      "total_amount : 398521.96\n",
      "pickup_location_id : 265\n",
      "dropoff_location_id : 265\n",
      "pickup_year : 2053\n",
      "pickup_month : 12\n",
      "\n",
      "==== Correlation with Tip Amount ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_id: -0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime: -0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_datetime: -0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count: -0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_distance: 0.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_code: 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment_type: -0.5048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount: 0.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mta_tax: -0.1362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolls_amount: 0.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imp_surcharge: 0.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_amount: 0.0882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_location_id: -0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_location_id: -0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_year: -0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_month: -0.0172\n",
      "\n",
      "==== Percentiles (25%, 50%, 75%) for Numeric Features ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_id -> 25%: 2496140, 50% (median): 4994989, 75%: 7497136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vendor_id -> 25%: 1, 50% (median): 2, 75%: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_datetime -> 25%: 1521729819000, 50% (median): 1528910757000, 75%: 1538042749000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropoff_datetime -> 25%: 1521737358000, 50% (median): 1528909736000, 75%: 1538033772000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count -> 25%: 1, 50% (median): 1, 75%: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_distance -> 25%: 5.83, 50% (median): 8.49, 75%: 11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_code -> 25%: 1, 50% (median): 1, 75%: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment_type -> 25%: 1, 50% (median): 1, 75%: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare_amount -> 25%: 23.5, 50% (median): 28.5, 75%: 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra -> 25%: 0.0, 50% (median): 0.0, 75%: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mta_tax -> 25%: 0.5, 50% (median): 0.5, 75%: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tolls_amount -> 25%: 0.0, 50% (median): 0.0, 75%: 5.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imp_surcharge -> 25%: 0.3, 50% (median): 0.3, 75%: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (39 + 1) / 72]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 102:==============================>                        (40 + 1) / 72]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m==== Percentiles (25\u001b[39m\u001b[33m%\u001b[39m\u001b[33m, 50\u001b[39m\u001b[33m%\u001b[39m\u001b[33m, 75\u001b[39m\u001b[33m%\u001b[39m\u001b[33m) for Numeric Features ====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m numeric_cols:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     percentiles = \u001b[43mtaxi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpercentile_approx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> 25%: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentiles[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 50% (median): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentiles[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 75%: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentiles[\u001b[32m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# --- Step 7: Explore Categorical Columns ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1263\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \n\u001b[32m   1245\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:=============================================>         (60 + 1) / 72]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# --- Step 1: Split pickup_date into year and month ---\n",
    "taxi = taxi.withColumn('pickup_year', F.substring('pickup_date', 1, 4).cast('int'))\n",
    "taxi = taxi.withColumn('pickup_month', F.substring('pickup_date', 6, 2).cast('int'))\n",
    "taxi = taxi.drop('pickup_date')\n",
    "\n",
    "# --- Step 2: Cast necessary columns from string to double ---\n",
    "string_cols = ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'imp_surcharge', 'total_amount']\n",
    "for col_name in string_cols:\n",
    "    taxi = taxi.withColumn(col_name, F.col(col_name).cast('double'))\n",
    "\n",
    "# --- Step 3: Check for missing or invalid values ---\n",
    "print(\"\\n==== Null / Missing Values Check ====\\n\")\n",
    "null_counts = taxi.select([F.count(F.when(F.col(c).isNull() | F.isnan(c) | (F.col(c) == ''), c)).alias(c) for c in taxi.columns])\n",
    "null_counts.show(vertical=True)\n",
    "\n",
    "# --- Step 4: Basic statistics ---\n",
    "stats = taxi.describe().toPandas()\n",
    "\n",
    "print(\"\\n==== Summary Statistics ====\\n\")\n",
    "for i, row in stats.iterrows():\n",
    "    print(f\"\\n{row['summary'].capitalize()}:\")\n",
    "    for col in stats.columns[1:]:\n",
    "        print(f\"{col} : {row[col]}\")\n",
    "\n",
    "# --- Step 5: Correlation Analysis (numerical features only) ---\n",
    "numeric_cols = [field for (field, dtype) in taxi.dtypes if dtype in ['int', 'bigint', 'double'] and field != 'tip_amount']\n",
    "\n",
    "# Assemble numeric features into a feature vector\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features_numeric\")\n",
    "taxi_vector = assembler.transform(taxi).select('features_numeric', 'tip_amount')\n",
    "\n",
    "# Compute Pearson correlation matrix\n",
    "print(\"\\n==== Correlation with Tip Amount ====\\n\")\n",
    "for feature in numeric_cols:\n",
    "    corr = taxi.stat.corr(feature, 'tip_amount')\n",
    "    if corr is not None:\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# --- Step 6: Distribution analysis (percentiles) ---\n",
    "print(\"\\n==== Percentiles (25%, 50%, 75%) for Numeric Features ====\\n\")\n",
    "for col_name in numeric_cols:\n",
    "    percentiles = taxi.select(F.percentile_approx(col_name, [0.25, 0.5, 0.75], 1000)).collect()[0][0]\n",
    "    print(f\"{col_name} -> 25%: {percentiles[0]}, 50% (median): {percentiles[1]}, 75%: {percentiles[2]}\")\n",
    "\n",
    "# --- Step 7: Explore Categorical Columns ---\n",
    "print(\"\\n==== Categorical Columns ====\\n\")\n",
    "print(\"\\nStore and Forward Flag distinct values:\")\n",
    "taxi.select('store_and_fwd_flag').distinct().show()\n",
    "\n",
    "print(\"\\nPayment Type distinct values:\")\n",
    "taxi.select('payment_type').distinct().show()\n",
    "\n",
    "print(\"\\nRate Code distinct values:\")\n",
    "taxi.select('rate_code').distinct().show()\n",
    "\n",
    "# --- Step 8: Final Data Types ---\n",
    "print(\"\\n==== Final Data Types after Processing ====\\n\")\n",
    "for name, dtype in taxi.dtypes:\n",
    "    print(f\"{name}: {dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f712ddd-85dd-4c1c-83c5-c421b4d86ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Transformer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b2f5c6-441c-473b-b64c-55c7f01cc4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:====================================================>  (69 + 1) / 72]\r"
     ]
    }
   ],
   "source": [
    "# Step 1: Decompose pickup_datetime and other time-related features into cyclical components\n",
    "taxi = taxi.withColumn(\"pickup_timestamp\", F.from_unixtime(F.col(\"pickup_datetime\"))) \\\n",
    "    .withColumn(\"pickup_year\", F.year(\"pickup_timestamp\")) \\\n",
    "    .withColumn(\"pickup_month\", F.month(\"pickup_timestamp\")) \\\n",
    "    .withColumn(\"pickup_day\", F.dayofmonth(\"pickup_timestamp\")) \\\n",
    "    .withColumn(\"pickup_hour\", F.hour(\"pickup_timestamp\")) \\\n",
    "    .withColumn(\"pickup_minute\", F.minute(\"pickup_timestamp\")) \\\n",
    "    .withColumn(\"pickup_second\", F.second(\"pickup_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68cc488f-75d4-493b-bba2-c43b6d327db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:======================================================>(71 + 1) / 72]\r"
     ]
    }
   ],
   "source": [
    "# Step 2: Build a custom cyclical transformer\n",
    "class CyclicalTransformer(Transformer):\n",
    "    def __init__(self, inputCol=None, maxVal=None):\n",
    "        super(CyclicalTransformer, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.maxVal = maxVal\n",
    "\n",
    "    def _transform(self, df):\n",
    "        return df.withColumn(f\"{self.inputCol}_sin\", F.sin(2 * math.pi * F.col(self.inputCol) / self.maxVal)) \\\n",
    "                 .withColumn(f\"{self.inputCol}_cos\", F.cos(2 * math.pi * F.col(self.inputCol) / self.maxVal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e276cb2d-6851-49aa-8594-66020c69bb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply cyclical transformations to time columns\n",
    "month_cyclical = CyclicalTransformer(inputCol=\"pickup_month\", maxVal=12)\n",
    "day_cyclical = CyclicalTransformer(inputCol=\"pickup_day\", maxVal=31)\n",
    "hour_cyclical = CyclicalTransformer(inputCol=\"pickup_hour\", maxVal=24)\n",
    "minute_cyclical = CyclicalTransformer(inputCol=\"pickup_minute\", maxVal=60)\n",
    "second_cyclical = CyclicalTransformer(inputCol=\"pickup_second\", maxVal=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d778511-4ec6-48f2-82b5-b0d835e1229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = month_cyclical.transform(taxi)\n",
    "taxi = day_cyclical.transform(taxi)\n",
    "taxi = hour_cyclical.transform(taxi)\n",
    "taxi = minute_cyclical.transform(taxi)\n",
    "taxi = second_cyclical.transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed490ea-2874-4f83-add2-e4624d674a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Select relevant features and drop unnecessary ones based on correlation\n",
    "# Based on the correlation, we will exclude:\n",
    "# 'trip_id', 'dropoff_datetime', 'passenger_count', vendor_id , 'pickup_location_id', 'dropoff_location_id',\n",
    "feature_cols = [\n",
    "    \"trip_distance\",\n",
    "    \"payment_type\",\n",
    "    \"rate_code\", \n",
    "    \"mta_tax\", \n",
    "    \"tolls_amount\", \n",
    "    \"imp_surcharge\",\n",
    "    \"total_amount\",\n",
    "    \"pickup_year\",\n",
    "    \"pickup_month_sin\", \"pickup_month_cos\",\n",
    "    \"pickup_day_sin\", \"pickup_day_cos\",\n",
    "    \"pickup_hour_sin\", \"pickup_hour_cos\",\n",
    "    \"pickup_minute_sin\", \"pickup_minute_cos\",\n",
    "    \"pickup_second_sin\", \"pickup_second_cos\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d13972-0719-4e76-a070-d8374f59b062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_id', 'int'),\n",
       " ('vendor_id', 'int'),\n",
       " ('pickup_datetime', 'bigint'),\n",
       " ('dropoff_datetime', 'bigint'),\n",
       " ('passenger_count', 'int'),\n",
       " ('trip_distance', 'double'),\n",
       " ('rate_code', 'int'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('payment_type', 'int'),\n",
       " ('fare_amount', 'double'),\n",
       " ('extra', 'double'),\n",
       " ('mta_tax', 'double'),\n",
       " ('tip_amount', 'double'),\n",
       " ('tolls_amount', 'double'),\n",
       " ('imp_surcharge', 'double'),\n",
       " ('total_amount', 'double'),\n",
       " ('pickup_location_id', 'int'),\n",
       " ('dropoff_location_id', 'int'),\n",
       " ('pickup_year', 'int'),\n",
       " ('pickup_month', 'int'),\n",
       " ('pickup_timestamp', 'string'),\n",
       " ('pickup_day', 'int'),\n",
       " ('pickup_hour', 'int'),\n",
       " ('pickup_minute', 'int'),\n",
       " ('pickup_second', 'int'),\n",
       " ('pickup_month_sin', 'double'),\n",
       " ('pickup_month_cos', 'double'),\n",
       " ('pickup_day_sin', 'double'),\n",
       " ('pickup_day_cos', 'double'),\n",
       " ('pickup_hour_sin', 'double'),\n",
       " ('pickup_hour_cos', 'double'),\n",
       " ('pickup_minute_sin', 'double'),\n",
       " ('pickup_minute_cos', 'double'),\n",
       " ('pickup_second_sin', 'double'),\n",
       " ('pickup_second_cos', 'double')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71bafe-c88b-47d2-accc-a9ca27f3a95d",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8586e7-29c1-47f6-9db2-22243631b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.0,0.433646...| 11.2|\n",
      "|[0.0,0.0,-0.51840...| 4.95|\n",
      "|[0.0,0.0,-0.55240...| 5.55|\n",
      "|[0.0,0.0,-0.34839...| 6.65|\n",
      "|[0.0,0.0,0.110628...| 10.0|\n",
      "|[0.0,0.0,-0.33139...| 6.25|\n",
      "|[0.0,0.0,-0.65441...|  6.0|\n",
      "|[0.0,0.0,1.011678...| 8.75|\n",
      "|[0.0,0.0,0.348641...|  7.8|\n",
      "|[0.0,0.0,0.773665...|  6.0|\n",
      "|[0.0,0.0,0.722662...| 6.04|\n",
      "|[0.0,0.0,0.535651...| 6.95|\n",
      "|[0.0,0.0,0.042624...|  3.5|\n",
      "|[0.0,0.0,-0.04237...| 5.35|\n",
      "|[0.0,0.0,-0.07638...|  6.4|\n",
      "|[0.0,0.0,0.110628...|  7.0|\n",
      "|[0.0,0.0,-0.73941...|  2.5|\n",
      "|[0.0,0.0,0.620656...| 9.96|\n",
      "|[0.0,0.0,-0.58640...|  7.3|\n",
      "|[0.0,0.0,0.042624...| 5.75|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Define categorical columns\n",
    "categorical_columns = [\"rate_code\", \"payment_type\"]\n",
    "\n",
    "# Step 2: Define numerical columns\n",
    "numerical_columns = [\n",
    "    \"trip_distance\", \"tolls_amount\", \"imp_surcharge\", \n",
    "    \"total_amount\", \"pickup_year\", \"pickup_month_sin\", \"pickup_month_cos\", \n",
    "    \"pickup_day_sin\", \"pickup_day_cos\", \"pickup_hour_sin\", \"pickup_hour_cos\", \n",
    "    \"pickup_minute_sin\", \"pickup_minute_cos\"# , \"pickup_second_sin\", \"pickup_second_cos\"\n",
    "]\n",
    "\n",
    "for c in numerical_columns:\n",
    "    taxi = taxi.withColumn(c, col(c).cast(DoubleType()))\n",
    "taxi = taxi.withColumn(\"tip_amount\", col(\"tip_amount\").cast(DoubleType()))\n",
    "\n",
    "# Step 3: Create StringIndexer for categorical features\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\") for col in categorical_columns]\n",
    "\n",
    "# Step 4: Create VectorAssembler for numerical features\n",
    "assembler = VectorAssembler(inputCols=numerical_columns, outputCol=\"numerical_features\")\n",
    "\n",
    "# Step 5: Apply StandardScaler to scale the numerical features\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Step 6: Create a pipeline to apply both StringIndexer for categorical features and StandardScaler for numerical features\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler])\n",
    "\n",
    "# Step 7: Fit and transform the data\n",
    "scaling_and_encoding_model = pipeline.fit(taxi)\n",
    "taxi_transformed = scaling_and_encoding_model.transform(taxi)\n",
    "\n",
    "# Step 8: Combine scaled features and encoded categorical features into a single vector\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_index\" for col in categorical_columns] + [\"scaled_features\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Apply final assembler to create the 'features' column\n",
    "final_df = final_assembler.transform(taxi_transformed)\n",
    "\n",
    "# Step 9: Select only the 'features' and rename 'tip_amount' to 'label'\n",
    "final_df = final_df.select(\"features\", col(\"tip_amount\").alias(\"label\"))\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09c495b-e97d-486f-a2fb-af24489073af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('features', 'vector'), ('label', 'double')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc712f55-7c64-45a0-8751-83dfb740b636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4fd3df2-8f31-42ed-8f0f-7dfcb795a5e8",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf565429-8c11-4108-be6d-2fc00675a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=final_df\n",
    "(train_data, test_data) = transformed.randomSplit([0.7, 0.3], seed = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54a60a81-79f3-4a58-bf27-5331450869ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(command):\n",
    "    import os\n",
    "    return os.popen(command).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca00fb-a991-47d0-8d09-75e5604ed694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d78dc54-82a7-41c4-9915-31321264e3a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > ../data/train.json\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > ../data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fecd52-189d-45af-841c-47819b311918",
   "metadata": {
    "tags": []
   },
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daffa79-5f41-4e78-aa1a-c914f8b3f1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9892ef1c-f7c4-4acd-8d7d-5630f0823c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 17:05:44 WARN Instrumentation: [a09401f2] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/05/04 17:06:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/04 17:06:53 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Create Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1396-77d4-4dd8-8777-d0e6fcbd629f",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a0f1ff-a175-4e00-ac1b-a5476dd7dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|       prediction|\n",
      "+-----+-----------------+\n",
      "|  0.0|7.836390419527919|\n",
      "|  0.0|7.837974345419144|\n",
      "|  0.0|7.829877867200599|\n",
      "|  0.0|4.502877050142552|\n",
      "|  0.0| 4.50021801538909|\n",
      "|  0.0|4.503570411441559|\n",
      "|  0.0|4.488905024721694|\n",
      "|  0.0|4.490926347873421|\n",
      "| 0.25|4.502323500594206|\n",
      "| 0.49|4.507184464598853|\n",
      "| 0.49|4.500972632151903|\n",
      "| 0.49|4.499051427753117|\n",
      "| 0.49|4.497901868125037|\n",
      "| 0.49|4.493495394995312|\n",
      "|  0.0|4.495943145751278|\n",
      "|  0.0|4.503616676738039|\n",
      "|  0.0|4.498549495817342|\n",
      "|  0.0|4.497380772864993|\n",
      "|  0.0|4.496338413572973|\n",
      "|  0.0|4.498158738215477|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "predictions.select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e97403-59a0-4ec4-b044-678ba6aa2b66",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6b166ca-8b62-4995-adca-129021dd6d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:==================================================>    (66 + 2) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.6724492908058566\n",
      "R^2 on test data = 0.4254532595381808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse = evaluator1_rmse.evaluate(predictions)\n",
    "r2 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse))\n",
    "print(\"R^2 on test data = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e075-c37c-492a-b894-2f64d4dea72b",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e04437d-2bf9-4803-83cc-b7ed17d311e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='LinearRegression_e536c8e7c678', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='featuresCol', doc='features column name.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='fitIntercept', doc='whether to fit an intercept term.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='labelCol', doc='label column name.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='maxIter', doc='max number of iterations (>= 0).'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='predictionCol', doc='prediction column name.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='regParam', doc='regularization parameter (>= 0).'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='standardization', doc='whether to standardize the training features before fitting the model.'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'),\n",
       " Param(parent='LinearRegression_e536c8e7c678', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f4388f9-db94-423c-879d-392c90041d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 261:======================================================>(71 + 1) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: regParam=0.1, elasticNet=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "print(f\"Best parameters: regParam={bestModel.getRegParam()}, elasticNet={bestModel.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b296a34-f0bb-4e36-a5c4-52740ef8cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: regParam=0.1, elasticNet=1.0\n"
     ]
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "print(f\"Best parameters: regParam={bestModel.getRegParam()}, elasticNet={bestModel.getElasticNetParam()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27e31c-959e-438c-bec0-1f109d7e2c15",
   "metadata": {},
   "source": [
    "## Best model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d86fc18-eb29-4f03-bfef-ed3aa1daa4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_eb028268bea9', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
      " Param(parent='LinearRegression_eb028268bea9', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='LinearRegression_eb028268bea9', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
      " Param(parent='LinearRegression_eb028268bea9', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='LinearRegression_eb028268bea9', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='LinearRegression_eb028268bea9', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
      " Param(parent='LinearRegression_eb028268bea9', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f666a-f792-4b50-b38f-b39f9208ab25",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e562da25-53d1-46dc-ac5d-68a6e9041927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 04:32:57 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state KILLED! Check the YARN application logs for more details.\n",
      "25/05/05 04:32:57 ERROR YarnClientSchedulerBackend: Diagnostics message: Application is killed by ResourceManager as it has exceeded the lifetime period.\n",
      "25/05/05 04:32:57 WARN TransportChannelHandler: Exception in connection from /10.100.30.58:46386\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/05 04:32:57 ERROR TransportResponseHandler: Still have 2 requests outstanding when connection from /10.100.30.58:46386 is closed\n",
      "25/05/05 04:32:57 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 10.100.30.58:46446, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/05 04:32:57 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/05 04:32:57 ERROR YarnScheduler: Lost executor 1 on hadoop-02.uni.innopolis.ru: Executor Process Lost\n",
      "25/05/05 04:32:57 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:847)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:114)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:178)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:125)\n",
      "Caused by: java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "model1.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model1 ../models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b220-54e1-49ce-802c-fc005d8f63c3",
   "metadata": {},
   "source": [
    "## Predict for test data using best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf8a05de-dcae-4bb0-8cf1-de760a62e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 267:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|            features|label|       prediction|\n",
      "+--------------------+-----+-----------------+\n",
      "|[0.0,0.0,-1.50446...|  0.0|6.747157660411386|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.601947217104696|\n",
      "|[0.0,0.0,-1.50446...| 0.01|4.601957437294171|\n",
      "|[0.0,0.0,-1.50446...| 0.05|4.601998318052072|\n",
      "|[0.0,0.0,-1.50446...|  0.1|4.602049418999448|\n",
      "|[0.0,0.0,-1.50446...| 0.17|4.602120960325774|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...| 0.49|4.602448006388978|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.602458226578453|\n",
      "|[0.0,0.0,-1.50446...|  0.0|4.602458226578453|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "096936c5-a575-48b7-92d2-be22893e892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > ../output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9a43d-80ce-4ec1-930f-671a240673d9",
   "metadata": {},
   "source": [
    "## Evaluate the best model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d0cc0e6-7d0d-4030-a372-375e5e9b84f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 271:===================================================>   (68 + 2) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.6681455890473145\n",
      "R^2 on test data = 0.40207776140191664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "r21 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "print(\"R^2 on test data = {}\".format(r21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308587c-3e52-41c6-ba7d-494d53cc45b1",
   "metadata": {},
   "source": [
    "# Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929b5a0-a159-4c94-ae56-086d3cf52df0",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d8981a-e274-4ab7-9d03-fefdc068b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Create Linear Regression Model\n",
    "gbt = GBTRegressor()\n",
    "\n",
    "# Fit the data to the pipeline stages\n",
    "model_gbt = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c9a67-fe61-49fe-898f-017df0c4006b",
   "metadata": {},
   "source": [
    "## Predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cb95001-9d15-4da4-98df-2b87c0c39200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 217:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|            features|label|       prediction|\n",
      "+--------------------+-----+-----------------+\n",
      "|[0.0,0.0,-1.50446...| 0.02|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.02|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.16|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.49|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.49|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.49|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...| 0.49|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "|[0.0,0.0,-1.50446...|  0.0|0.883268410047259|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model_gbt.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94da6f-f913-45ec-a377-d9c42dacb075",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3508997-af5a-4882-8fd6-8d0a5be07ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:======================================================>(71 + 1) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.576290415629213\n",
      "R^2 on test data = 0.7141985849993504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2d06-39bd-4c71-b0cc-18a63d995949",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b99e50b9-5090-45ed-bfa7-7ddca0706176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='GBTRegressor_326f2a155795', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='featuresCol', doc='features column name.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='labelCol', doc='label column name.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='maxIter', doc='max number of iterations (>= 0).'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='predictionCol', doc='prediction column name.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='seed', doc='random seed.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='validationIndicatorCol', doc='name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'),\n",
       " Param(parent='GBTRegressor_326f2a155795', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gbt.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dea6a22-9f7a-483b-818e-0b4e62f723e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 03:38:38 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/04 03:38:38 WARN CacheManager: Asked to cache already cached data.\n",
      "25/05/04 04:20:06 WARN BlockManagerMaster: Failed to remove RDD 3586 - Block rdd_3586_63 does not exist\n",
      "org.apache.spark.SparkException: Block rdd_3586_63 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2093)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2057)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1993)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/04 04:20:06 ERROR ContextCleaner: Error cleaning RDD 3586\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:196)\n",
      "\tat org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:2070)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupRDD(ContextCleaner.scala:225)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:200)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Caused by: org.apache.spark.SparkException: Block rdd_3586_63 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:318)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:269)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:547)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2093)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:2057)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeRdd$4$adapted(BlockManager.scala:1993)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeRdd(BlockManager.scala:1993)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$2(BlockManagerStorageEndpoint.scala:53)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:101)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GBTRegressionModel: uid=GBTRegressor_9b18197766ee, numTrees=20, numFeatures=15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "    .addGrid(gbt.stepSize, [0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42,\n",
    "    collectSubModels=False\n",
    ")\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524320f-8764-4ed4-b8ff-9d0a43b2ee29",
   "metadata": {},
   "source": [
    "## Best model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccafef7f-5a51-4370-8572-447f06b92248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='GBTRegressor_9b18197766ee', name='validationTol', doc='Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used.'): 0.01,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='seed', doc='random seed.'): -8040837446833772744,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='stepSize', doc='Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator.'): 0.1,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='lossType', doc='Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute'): 'squared',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='maxIter', doc='max number of iterations (>= 0).'): 20,\n",
      " Param(parent='GBTRegressor_9b18197766ee', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'all'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a3f34-cd0b-4e7e-a0ff-8670bee1e2ef",
   "metadata": {},
   "source": [
    "## Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "098bb094-67c3-4a8b-a52a-95a1f7c7bc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 16:23:56 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model2 ../models/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220eee3-99d3-44cc-82ab-b941fd5e5e7c",
   "metadata": {},
   "source": [
    "## Predict for test data using best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55451438-dea4-4765-977b-d0753fd134e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressionModel\n",
    "\n",
    "# Load the model\n",
    "model2 = GBTRegressionModel.load(\"project/models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53789d15-33a1-43b2-8e8e-df7627df5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+\n",
      "|            features|label|       prediction|\n",
      "+--------------------+-----+-----------------+\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...|  0.0|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.01|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.08|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.09|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "|[0.0,0.0,-1.50446...| 0.49|1.075400044122528|\n",
      "+--------------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52df5365-5d86-407f-b71a-878fff80b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions.csv/*.csv > ../output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9e394-a65a-4cb3-b9c8-1036fd3a443a",
   "metadata": {},
   "source": [
    "## Evaluate the best model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e83b832-0088-4e95-95fc-768bc13d70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=====================================================> (70 + 2) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.552106406030934\n",
      "R^2 on test data = 0.7310987322461698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r22 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R^2 on test data = {}\".format(r22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd7b48-f536-495a-ac38-90ea1b9014af",
   "metadata": {},
   "source": [
    "# Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b91f3c3-50ca-4ce1-9dfa-dba7449fbc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 274:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+------------------+-------------------+\n",
      "|model                                                                         |RMSE              |R2                 |\n",
      "+------------------------------------------------------------------------------+------------------+-------------------+\n",
      "|LinearRegressionModel: uid=LinearRegression_eb028268bea9, numFeatures=15      |3.6681455890473145|0.40207776140191664|\n",
      "|GBTRegressionModel: uid=GBTRegressor_9b18197766ee, numTrees=20, numFeatures=15|2.552106406030934 |0.7310987322461698 |\n",
      "+------------------------------------------------------------------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "models = [[str(model1),rmse1, r21], [str(model2),rmse2, r22]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e008d30b-9489-44a2-84a0-95bcd9608319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > ../output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1073203-5cdb-4355-bc59-b8f6705a9d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
